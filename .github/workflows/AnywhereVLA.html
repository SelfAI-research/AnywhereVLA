<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation</title>
  <style>
    body {
      margin: 0;
      font-family: Arial, sans-serif;
      background-color: #ffffff;
      color: #000000;
    }
    .header {
      background-color: #f5f5f5;
      padding: 20px;
      text-align: center;
    }
    .header h1 {
      margin: 0;
      font-size: 2em;
    }
    .authors {
      font-size: 1.2em;
      margin: 10px 0;
    }
    .contribution {
      font-size: 1em;
      margin-bottom: 10px;
    }
    .institution {
      font-size: 1.1em;
      margin-bottom: 20px;
    }
    .links {
      display: flex;
      justify-content: center;
      gap: 20px;
    }
    .link-box {
      width: 100px;
      height: 100px;
      background-color: #ffffff;
      border: 1px solid #ddd;
      display: flex;
      align-items: center;
      justify-content: center;
      border-radius: 5px;
      transition: transform 0.2s;
    }
    .link-box:hover {
      transform: scale(1.05);
    }
    .link-box img {
      max-width: 80%;
      max-height: 80%;
    }
    .content {
      padding: 40px 0;
    }
    figure {
      width: 100%;
      margin: 60px 0;
    }
    .media-container {
      max-width: 80%;
      margin: 0 auto;
      text-align: left;
    }
    video,
    img {
      display: block;
      width: 100%;
      height: auto;
      border-radius: 8px;
    }
    figcaption {
      font-style: normal;
      font-size: 1em;
      margin-top: 10px;
      width: 100%;
    }
    .caption-left {
      text-align: left;
    }
    .caption-justify {
      text-align: justify;
    }
    h2 {
      text-align: left;
      max-width: 80%;
      margin: 60px auto 20px;
      font-size: 1.8em;
    }
    .bibtex {
      background-color: #f5f5f5;
      padding: 20px;
      font-family: monospace;
      text-align: left;
      white-space: pre;
      max-width: 80%;
      margin: 40px auto;
      border-radius: 8px;
    }
  </style>
</head>
<body>
  <div class="header">
    <h1>AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation</h1>
    <div class="authors">
      Konstantin Gubernatorov*, Artem Voronov*, Roman Voronov*, Sergei Pasynkov*, Stepan
      Perminov, Ziang Guo, and Dzmitry Tsetserukou
    </div>
    <div class="contribution">*Equal contribution</div>
    <div class="institution">Skolkovo Institute of Science and Technology</div>
    <div class="links">
      <a
        href="https://github.com/SelfAI-research/AnywhereVLA"
        class="link-box"
        target="_blank"
      >
        <img
          src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png"
          alt="GitHub Logo"
        />
      </a>
      <a href="https://arxiv.org/placeholder" class="link-box" target="_blank">
        <img src="arxiv.svg" alt="arXiv Logo" />
      </a>
    </div>
  </div>

  <div class="content">
    <figure>
      <div class="media-container">
        <video controls>
          <source src="figures/anywhereVLA5x.mp4" type="video/mp4" />
          Your browser does not support the video tag.
        </video>
        <figcaption class="caption-left">
          We introduce AnywhereVLA, modular pipeline to perform mobile manipulation in large-scale
          novel dynamic indoor environments with just one language command.
        </figcaption>
      </div>
    </figure>

    <h2>AnywhereVLA Architecture</h2>
    <figure>
      <div class="media-container">
        <img src="figures/architecture.png" alt="AnywhereVLA Architecture" />
        <figcaption class="caption-justify">
          AnywhereVLA is the modular architecture comprising VLA manipulation and environment
          exploration. Given the task, AnywhereVLA parses it into simpler actions which further
          condition Active Environment Exploration. Exploration and navigation in larger-scale
          indoor environments are performed within a 3D point cloud semantic map. By leveraging a
          purpose-built pick-and-place dataset, AnywhereVLA exhibits robust generalization
          capacities.
        </figcaption>
      </div>
    </figure>

    <h2>Applications</h2>
    <figure>
      <div class="media-container">
        <img src="figures/teaser.png" alt="Applications" />
        <figcaption class="caption-left">Use cases.</figcaption>
      </div>
    </figure>

    <h2>BibTex</h2>
    <div class="bibtex">
@article{gubernatorov2025anywherevla,<br>
  title={AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation},<br>
  author={Gubernatorov, Konstantin and Voronov, Artem and Voronov, Roman and Pasynkov, Sergei and Perminov, Stepan and Guo, Ziang and Tsetserukou, Dzmitry},<br>
  journal={arXiv preprint arXiv:2509.21006},<br>
  year={2025}<br>
}
    </div>
  </div>
</body>
</html>
