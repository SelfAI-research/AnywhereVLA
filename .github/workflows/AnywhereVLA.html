<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <style>
    :root {
      --primary-color: #2563eb;
      --secondary-color: #64748b;
      --text-color: #1e293b;
      --text-light: #64748b;
      --background: #ffffff;
      --surface: #f8fafc;
      --border: #e2e8f0;
      --shadow: 0 1px 3px 0 rgb(0 0 0 / 0.1), 0 1px 2px -1px rgb(0 0 0 / 0.1);
      --shadow-lg: 0 10px 15px -3px rgb(0 0 0 / 0.1), 0 4px 6px -4px rgb(0 0 0 / 0.1);
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background-color: var(--background);
      color: var(--text-color);
      line-height: 1.6;
      font-size: 16px;
    }

    .container {
      max-width: 1000px;
      margin: 0 auto;
      padding: 0 20px;
    }

    /* Header Section */
    .header {
      background: linear-gradient(135deg, var(--surface) 0%, #ffffff 100%);
      padding: 60px 0;
      border-bottom: 1px solid var(--border);
    }

    .header-content {
      text-align: center;
    }

    .title {
      font-size: clamp(2.5rem, 5vw, 3.5rem);
      font-weight: 700;
      margin: 0 0 20px 0;
      background: linear-gradient(135deg, var(--primary-color), #7c3aed);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      line-height: 1.2;
    }

    .subtitle {
      font-size: 1.25rem;
      color: var(--text-light);
      margin-bottom: 30px;
      font-weight: 400;
    }

    .authors {
      font-size: 1.1rem;
      margin: 20px 0;
      color: var(--text-color);
      font-weight: 500;
    }

    .contribution {
      font-size: 0.95rem;
      margin-bottom: 15px;
      color: var(--text-light);
      font-style: italic;
    }

    .institution {
      font-size: 1rem;
      margin-bottom: 40px;
      color: var(--text-light);
      font-weight: 500;
    }

    .links {
      display: flex;
      justify-content: center;
      gap: 20px;
      flex-wrap: wrap;
    }

    .link-button {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 12px 24px;
      background: var(--background);
      border: 2px solid var(--border);
      border-radius: 12px;
      text-decoration: none;
      color: var(--text-color);
      font-weight: 500;
      transition: all 0.2s ease;
      box-shadow: var(--shadow);
    }

    .link-button:hover {
      border-color: var(--primary-color);
      color: var(--primary-color);
      transform: translateY(-2px);
      box-shadow: var(--shadow-lg);
    }

    .link-button img {
      width: 20px;
      height: 20px;
    }

    /* Main Content */
    .content {
      padding: 80px 0;
    }

    .section {
      margin-bottom: 80px;
    }

    .section-title {
      font-size: 2rem;
      font-weight: 600;
      margin-bottom: 30px;
      color: var(--text-color);
      text-align: center;
    }

    /* Method Section */
    .method-content {
      background: var(--surface);
      padding: 30px;
      border-radius: 12px;
      margin-bottom: 40px;
    }

    .method-text {
      font-size: 1rem;
      line-height: 1.7;
      color: var(--text-color);
      text-align: justify;
    }

    /* Abstract Section */
    .abstract {
      background: var(--surface);
      padding: 40px;
      border-radius: 16px;
      border-left: 4px solid var(--primary-color);
      margin-bottom: 60px;
    }

    .abstract-title {
      font-size: 1.5rem;
      font-weight: 600;
      margin-bottom: 20px;
      color: var(--text-color);
    }

    .abstract-text {
      font-size: 1.1rem;
      line-height: 1.7;
      color: var(--text-color);
      text-align: justify;
    }

    /* Media Elements */
    .media-container {
      max-width: 100%;
      margin: 0 auto;
    }

    .hero-video {
      width: 100%;
      border-radius: 16px;
      box-shadow: var(--shadow-lg);
      margin-bottom: 20px;
    }

    .figure {
      margin: 60px 0;
      text-align: center;
    }

    .figure img {
      max-width: 100%;
      height: auto;
      border-radius: 12px;
      box-shadow: var(--shadow);
    }

    .gif-container {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 20px;
      margin: 40px 0;
    }

    .gif-item {
      text-align: center;
    }

    .gif-item.featured {
      grid-column: 1 / -1;
      margin: 40px 0;
    }

    .gif-item img {
      max-width: 100%;
      height: auto;
      border-radius: 12px;
      box-shadow: var(--shadow-lg);
      border: 2px solid var(--primary-color);
    }

    .gif-item.featured img {
      max-width: 80%;
      box-shadow: var(--shadow-lg);
      border: 3px solid var(--primary-color);
    }

    .gif-item.horizontal {
      grid-column: 1 / -1;
      margin-top: 20px;
    }

    .gif-item.horizontal img {
      max-width: 60%;
      box-shadow: var(--shadow-lg);
      border: 2px solid var(--primary-color);
    }

    .gif-caption {
      margin-top: 15px;
      font-size: 0.95rem;
      color: var(--text-light);
      font-weight: 500;
    }

    .gif-item.featured .gif-caption {
      font-size: 1.1rem;
      font-weight: 600;
      color: var(--text-color);
    }

    .figure-caption {
      margin-top: 20px;
      font-size: 1rem;
      color: var(--text-light);
      text-align: left;
      max-width: 800px;
      margin-left: auto;
      margin-right: auto;
    }

    .figure-caption.justify {
      text-align: justify;
    }

    /* BibTeX Section */
    .bibtex-section {
      background: var(--surface);
      padding: 40px;
      border-radius: 16px;
      margin-top: 60px;
    }

    .bibtex-container {
      position: relative;
      background: var(--background);
      border: 1px solid var(--border);
      border-radius: 12px;
      overflow: hidden;
    }

    .bibtex-header {
      background: var(--surface);
      padding: 15px 20px;
      border-bottom: 1px solid var(--border);
      display: flex;
      justify-content: space-between;
      align-items: center;
    }

    .bibtex-title {
      font-weight: 600;
      color: var(--text-color);
    }

    .copy-btn {
      background: var(--primary-color);
      color: white;
      border: none;
      padding: 8px 16px;
      border-radius: 8px;
      font-size: 0.9rem;
      cursor: pointer;
      transition: background-color 0.2s;
    }

    .copy-btn:hover {
      background: #1d4ed8;
    }

    .bibtex {
      padding: 20px;
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
      font-size: 0.9rem;
      line-height: 1.5;
      color: var(--text-color);
      overflow-x: auto;
      white-space: pre-wrap;
    }

    /* Responsive Design */
    @media (max-width: 768px) {
      .header {
        padding: 40px 0;
      }
      
      .content {
        padding: 40px 0;
      }
      
      .abstract {
        padding: 30px 20px;
      }
      
      .links {
        flex-direction: column;
        align-items: center;
      }
      
      .link-button {
        width: 200px;
        justify-content: center;
      }
    }

    /* Animations */
    @keyframes fadeInUp {
      from {
        opacity: 0;
        transform: translateY(30px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    .fade-in {
      animation: fadeInUp 0.6s ease-out;
    }
  </style>
</head>
<body>
  <div class="header">
    <div class="container">
      <div class="header-content fade-in">
        <h1 class="title">AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation</h1>
        <p class="subtitle">A modular pipeline for mobile manipulation in large-scale novel dynamic indoor environments</p>
        
        <div class="authors">
          Konstantin Gubernatorov*, Artem Voronov*, Roman Voronov*, Sergei Pasynkov*, Stepan Perminov, Ziang Guo, and Dzmitry Tsetserukou
        </div>
        <div class="contribution">*Equal contribution</div>
        <div class="institution">Skolkovo Institute of Science and Technology</div>
        
        <div class="links">
          <a href="https://github.com/SelfAI-research/AnywhereVLA" class="link-button" target="_blank">
            <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" alt="GitHub" />
            Code
          </a>
          <a href="https://arxiv.org/abs/2509.21006" class="link-button" target="_blank">
            <img src="https://raw.githubusercontent.com/SelfAI-research/AnywhereVLA/main/.github/workflows/arxiv.svg" alt="arXiv" />
            Paper
          </a>
          <a href="https://www.youtube.com/watch?v=SEddJMijuwA" class="link-button" target="_blank">
            <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
              <path d="M23.498 6.186a3.016 3.016 0 0 0-2.122-2.136C19.505 3.545 12 3.545 12 3.545s-7.505 0-9.377.505A3.017 3.017 0 0 0 .502 6.186C0 8.07 0 12 0 12s0 3.93.502 5.814a3.016 3.016 0 0 0 2.122 2.136c1.871.505 9.376.505 9.376.505s7.505 0 9.377-.505a3.015 3.015 0 0 0 2.122-2.136C24 15.93 24 12 24 12s0-3.93-.502-5.814zM9.545 15.568V8.432L15.818 12l-6.273 3.568z"/>
            </svg>
            Video
          </a>
        </div>
      </div>
    </div>
  </div>

  <div class="content">
    <div class="container">
      <!-- Hero Video Section -->
      <div class="section">
        <div class="media-container">
          <video class="hero-video" controls loop>
            <source src="https://raw.githubusercontent.com/SelfAI-research/AnywhereVLA/main/.github/workflows/anywhereVLA5x.mp4" type="video/mp4" />
          </video>
        </div>
      </div>

      <!-- Abstract Section -->
      <div class="section">
        <div class="abstract">
          <h2 class="abstract-title">Abstract</h2>
          <p class="abstract-text">
            We introduce AnywhereVLA, a modular pipeline designed to perform mobile manipulation in large-scale novel dynamic indoor environments with just one language command. Our system combines Vision-Language-Action (VLA) manipulation capabilities with active environment exploration, enabling robots to navigate and manipulate objects in previously unseen environments. By leveraging a purpose-built pick-and-place dataset and 3D point cloud semantic mapping, AnywhereVLA exhibits robust generalization capacities across diverse indoor scenarios. The modular architecture allows for seamless integration of exploration and manipulation tasks, making it suitable for real-world applications in dynamic environments.
          </p>
        </div>
      </div>

      <!-- Method Section -->
      <div class="section">
        <h2 class="section-title">Method</h2>
        <div class="method-content">
          <p class="method-text">
            <strong>AnywhereVLA</strong> takes a natural language command as input and simultaneously performs environment exploration while executing mobile manipulation tasks. Our system integrates several key components: (1) <strong>Command Interpreter</strong> that parses complex tasks into simpler actions, (2) <strong>Object Perception</strong> module using YOLO v12m for object detection and 2D-to-3D projection for semantic mapping, (3) <strong>Active Exploration</strong> system that guides navigation in unknown environments, and (4) <strong>VLA Manipulation</strong> module for precise object interaction. The system leverages 3D point cloud semantic mapping to maintain spatial understanding and enables robust generalization across diverse indoor scenarios.
          </p>
        </div>
      </div>

      <!-- Architecture Section -->
      <div class="section">
        <h2 class="section-title">System Architecture</h2>
        <div class="figure">
          <img src="https://raw.githubusercontent.com/SelfAI-research/AnywhereVLA/main/.github/workflows/architecture.png" alt="AnywhereVLA Architecture" />
          <div class="figure-caption justify">
            AnywhereVLA is the modular architecture comprising VLA manipulation and environment exploration. Given the task, AnywhereVLA parses it into simpler actions which further condition Active Environment Exploration. Exploration and navigation in larger-scale indoor environments are performed within a 3D point cloud semantic map. By leveraging a purpose-built pick-and-place dataset, AnywhereVLA exhibits robust generalization capacities.
          </div>
        </div>
      </div>

      <!-- Active SLAM Demonstration -->
      <div class="section">
        <h2 class="section-title">Active SLAM & Autonomous Exploration</h2>
        <div class="gif-container">
          <div class="gif-item featured">
            <img src="https://raw.githubusercontent.com/SelfAI-research/AnywhereVLA/main/.github/workflows/active_slam_autonomous_exploration_rviz_1920x1080.gif" alt="Active SLAM Autonomous Exploration RViz" />
            <div class="gif-caption">
              Real-time active SLAM visualization showing autonomous exploration in RViz. The system dynamically builds maps while planning optimal exploration paths.
            </div>
          </div>
        </div>
      </div>

      <!-- System Demonstrations -->
      <div class="section">
        <h2 class="section-title">System Demonstrations</h2>
        <div class="gif-container">
          <div class="gif-item">
            <img src="https://raw.githubusercontent.com/SelfAI-research/AnywhereVLA/main/.github/workflows/VLA_grap_the_bottle_720x1280.gif" alt="VLA Grab the Bottle" />
            <div class="gif-caption">
              VLA manipulation module successfully grasping and manipulating objects based on language commands.
            </div>
          </div>
          <div class="gif-item">
            <img src="https://raw.githubusercontent.com/SelfAI-research/AnywhereVLA/main/.github/workflows/active_slam_autonomous_exluaration_realrobot_720x1280.gif" alt="Active SLAM Real Robot" />
            <div class="gif-caption">
              Real robot performing autonomous exploration and mapping in dynamic indoor environments.
            </div>
          </div>
        </div>
        <div class="gif-container">
          <div class="gif-item horizontal">
            <img src="https://raw.githubusercontent.com/SelfAI-research/AnywhereVLA/main/.github/workflows/camera2map_projection_demonstration_800x430.gif" alt="Camera to Map Projection" />
            <div class="gif-caption">
              Camera-to-map projection demonstration showing 2D object detection mapped to 3D semantic map.
            </div>
          </div>
        </div>
      </div>

      <!-- Applications Section -->
      <div class="section">
        <h2 class="section-title">Applications</h2>
        <div class="figure">
          <img src="https://raw.githubusercontent.com/SelfAI-research/AnywhereVLA/main/.github/workflows/teaser.png" alt="Applications" />
          <div class="figure-caption">
            Use cases demonstrating AnywhereVLA's capabilities in various indoor manipulation scenarios including object retrieval, cleaning tasks, and delivery operations.
          </div>
        </div>
      </div>

      <!-- BibTeX Section -->
      <div class="bibtex-section">
        <h2 class="section-title">Citation</h2>
        <div class="bibtex-container">
          <div class="bibtex-header">
            <span class="bibtex-title">BibTeX</span>
            <button class="copy-btn" onclick="copyBibtex()">📋 Copy</button>
          </div>
          <div class="bibtex" id="bibtex-text">@article{gubernatorov2025anywherevla,
  title={AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation},
  author={Gubernatorov, Konstantin and Voronov, Artem and Voronov, Roman and Pasynkov, Sergei and Perminov, Stepan and Guo, Ziang and Tsetserukou, Dzmitry},
  journal={arXiv preprint arXiv:2509.21006},
  year={2025}
}</div>
        </div>
      </div>
    </div>
  </div>

  <script>
    function copyBibtex() {
      const text = document.getElementById("bibtex-text").innerText;
      navigator.clipboard.writeText(text);
      const btn = document.querySelector(".copy-btn");
      btn.textContent = "✅ Copied!";
      setTimeout(() => (btn.textContent = "📋 Copy"), 2000);
    }

    // Add smooth scrolling for better UX
    document.addEventListener('DOMContentLoaded', function() {
      // Add fade-in animation to sections as they come into view
      const observerOptions = {
        threshold: 0.1,
        rootMargin: '0px 0px -50px 0px'
      };

      const observer = new IntersectionObserver(function(entries) {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            entry.target.style.opacity = '1';
            entry.target.style.transform = 'translateY(0)';
          }
        });
      }, observerOptions);

      // Observe all sections
      document.querySelectorAll('.section').forEach(section => {
        section.style.opacity = '0';
        section.style.transform = 'translateY(30px)';
        section.style.transition = 'opacity 0.6s ease, transform 0.6s ease';
        observer.observe(section);
      });
    });
  </script>
</body>
</html>
