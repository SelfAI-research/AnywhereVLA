<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation</title>
    <style>
        body {
            margin: 0;
            font-family: Arial, sans-serif;
            background-color: #ffffff;
            color: #000000;
        }
        .header {
            background-color: #f5f5f5; /* Light grey background */
            padding: 20px;
            text-align: center;
        }
        .header h1 {
            margin: 0;
            font-size: 2em;
        }
        .authors {
            font-size: 1.2em;
            margin: 10px 0;
        }
        .contribution {
            font-size: 1em;
            margin-bottom: 10px;
        }
        .institution {
            font-size: 1.1em;
            margin-bottom: 20px;
        }
        .links {
            display: flex;
            justify-content: center;
            gap: 20px;
        }
        .link-box {
            width: 100px;
            height: 100px;
            background-color: #ffffff;
            border: 1px solid #ddd;
            display: flex;
            align-items: center;
            justify-content: center;
            border-radius: 5px;
            transition: transform 0.2s;
        }
        .link-box:hover {
            transform: scale(1.05);
        }
        .link-box img {
            max-width: 80%;
            max-height: 80%;
        }
        .content {
            padding: 40px;
            text-align: center;
        }
        figure {
            margin: 40px auto;
            text-align: center;
            display: flex;
            flex-direction: column;
            align-items: center;
            width: fit-content;
        }
        video, img {
            display: block;
            max-width: 100%;
            height: auto;
            border-radius: 8px;
        }
        figcaption {
            font-style: normal;
            text-align: center;
            font-size: 1em;
            margin-top: 10px;
            width: 100%;
        }
        /* Special style for architecture caption */
        .figcaption-architecture {
            text-align: justify;
            max-width: 80%;
            margin: 10px auto 0;
            font-style: normal;
        }
        h2 {
            text-align: left;
            max-width: 80%;
            margin: 60px auto 20px;
            font-size: 1.8em;
        }
        .bibtex {
            background-color: #f5f5f5;
            padding: 20px;
            font-family: monospace;
            text-align: left;
            white-space: pre;
            max-width: 80%;
            margin: 40px auto;
            border-radius: 8px;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation</h1>
        <div class="authors">
            Konstantin Gubernatorov*, Artem Voronov*, Roman Voronov*, Sergei Pasynkov*, Stepan Perminov, Ziang Guo, and Dzmitry Tsetserukou
        </div>
        <div class="contribution">*Equal contribution</div>
        <div class="institution">Skolkovo Institute of Science and Technology</div>
        <div class="links">
            <a href="https://github.com/SelfAI-research/AnywhereVLA" class="link-box" target="_blank">
                <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" alt="GitHub Logo">
            </a>
            <a href="https://arxiv.org/placeholder" class="link-box" target="_blank">
                <img src="arxiv.svg" alt="arXiv Logo">
            </a>
        </div>
    </div>

    <div class="content">
        <figure>
            <video controls>
                <source src=".github/workflows/anywhereVLA5x.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <figcaption>We introduce AnywhereVLA, modular pipeline to perform mobile manipulation in large-scale novel dynamic indoor environments with just one language command.</figcaption>
        </figure>

        <h2>AnywhereVLA Architecture</h2>
        <figure>
            <img src=".github/workflows/architecture.png" alt="AnywhereVLA Architecture">
            <figcaption class="figcaption-architecture">
                AnywhereVLA is the modular architecture comprising VLA manipulation and environment exploration. 
                Given the task, AnywhereVLA parses it into simpler actions which further condition Active Environment Exploration. 
                Exploration and navigation in larger-scale indoor environments are performed within a 3D point cloud semantic map. 
                By leveraging a purpose-built pick-and-place dataset, AnywhereVLA exhibits robust generalization capacities.
            </figcaption>
        </figure>

        <h2>Applications</h2>
        <figure>
            <img src=".github/workflows/teaser.png" alt="Applications">
            <figcaption>Use cases.</figcaption>
        </figure>

        <h2>BibTex</h2>
        <div class="bibtex">
@article{gubernatorov2025anywherevla,<br>
  title={AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation},<br>
  author={Gubernatorov, Konstantin and Voronov, Artem and Voronov, Roman and Pasynkov, Sergei and Perminov, Stepan and Guo, Ziang and Tsetserukou, Dzmitry},<br>
  journal={arXiv preprint arXiv:2509.21006},<br>
  year={2025}<br>
}
        </div>
    </div>
</body>
</html>
